{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from trixi.util import Config\n",
    "import matplotlib.pyplot as plt\n",
    "from experiment import BaselineExperiment, ToyDataset\n",
    "from util import plot_dependency_map\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test with toy dataset\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# define config\n",
    "c = Config()\n",
    "c.batch_size = 10\n",
    "c.n_epochs = 50\n",
    "c.learning_rate = 0.001\n",
    "if torch.cuda.is_available():\n",
    "    c.use_cuda = True\n",
    "else:\n",
    "    c.use_cuda = False\n",
    "c.rnd_seed = 2\n",
    "c.log_interval = 10\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "WARNING:root:Setting up a new session...\n",
      "Process Process-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/m167k/miniconda3/envs/torchenv/lib/python3.7/site-packages/multiprocess/process.py\", line 297, in _bootstrap\n    self.run()\n",
      "  File \"/home/m167k/miniconda3/envs/torchenv/lib/python3.7/site-packages/multiprocess/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/m167k/git_repos/trixi/trixi/logger/visdom/numpyvisdomlogger.py\", line 92, in __show\n    func, args, kwargs = queue.get()\n",
      "  File \"/home/m167k/miniconda3/envs/torchenv/lib/python3.7/site-packages/multiprocess/queues.py\", line 97, in get\n    res = self._recv_bytes()\n",
      "  File \"/home/m167k/miniconda3/envs/torchenv/lib/python3.7/site-packages/multiprocess/connection.py\", line 219, in recv_bytes\n    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/m167k/miniconda3/envs/torchenv/lib/python3.7/site-packages/multiprocess/connection.py\", line 410, in _recv_bytes\n    buf = self._recv(4)\n",
      "  File \"/home/m167k/miniconda3/envs/torchenv/lib/python3.7/site-packages/multiprocess/connection.py\", line 382, in _recv\n    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "exp = BaselineExperiment(config=c, name='experiment_baseline', n_epochs=c.n_epochs,\n",
    "                         seed=c.rnd_seed, base_dir='./experiment_dir',\n",
    "                         loggers={\"visdom\": [\"visdom\", {\"exp_name\": \"myenv\"}]})\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Config:\n",
      "{\n    \"base_dir\": \"./experiment_dir\",\n    \"batch_size\": 10,\n    \"learning_rate\": 0.001,\n    \"log_interval\": 10,\n    \"n_epochs\": 50,\n    \"name\": \"experiment_baseline\",\n    \"rnd_seed\": 2,\n    \"seed\": 2,\n    \"use_cuda\": true\n}\n",
      "Experiment set up.\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "INFO:default-i3mqYQlcRo:Config:\n",
      "INFO:default-i3mqYQlcRo:{\n    \"base_dir\": \"./experiment_dir\",\n    \"batch_size\": 10,\n    \"learning_rate\": 0.001,\n    \"log_interval\": 10,\n    \"n_epochs\": 50,\n    \"name\": \"experiment_baseline\",\n    \"rnd_seed\": 2,\n    \"seed\": 2,\n    \"use_cuda\": true\n}\n",
      "INFO:default-i3mqYQlcRo:Experiment set up.\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "<Figure size 1440x1440 with 9 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exp.setup()\n",
    "# run backpropagation for each dimension to compute what other\n",
    "# dimensions it depends on.\n",
    "n_pixels = 10\n",
    "x = (np.random.rand(1, 1, n_pixels, n_pixels) > 0.5).astype(np.float32)\n",
    "res = []\n",
    "exp.model.eval()\n",
    "for i in range(n_pixels):\n",
    "    for j in range(n_pixels):\n",
    "        xtr = torch.tensor(x, requires_grad=True, device='cuda:0')\n",
    "        xtrhat = exp.model(xtr).squeeze()[0]\n",
    "        loss = xtrhat[i, j]\n",
    "        loss.backward()\n",
    "        \n",
    "        depends = (xtr.grad[0].cpu().numpy() != 0).astype(np.uint8)\n",
    "        res.append(depends)\n",
    "\n",
    "# plot the dependency for a couple of pixels\n",
    "plot_dependency_map(res, [(0,0), (0,3), (0,6),\n",
    "                          (3,0), (3,3), (3,6),\n",
    "                          (6,0), (6,3), (6,6)])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Config:\n",
      "{\n    \"base_dir\": \"./experiment_dir\",\n    \"batch_size\": 10,\n    \"learning_rate\": 0.001,\n    \"log_interval\": 10,\n    \"n_epochs\": 50,\n    \"name\": \"experiment_baseline\",\n    \"rnd_seed\": 2,\n    \"seed\": 2,\n    \"use_cuda\": true\n}\n",
      "Experiment set up.\n",
      "Experiment started.\n",
      "log: Train Epoch: 0 [0/22 samples (0%)]\t Batch Loss: 1.110972\n",
      "\nValidation set: Average loss: 1.0766)\n\n",
      "log: Train Epoch: 1 [0/22 samples (0%)]\t Batch Loss: 0.837393\n",
      "\nValidation set: Average loss: 1.0544)\n\n",
      "log: Train Epoch: 2 [0/22 samples (0%)]\t Batch Loss: 0.646041\n",
      "\nValidation set: Average loss: 1.0195)\n\n",
      "log: Train Epoch: 3 [0/22 samples (0%)]\t Batch Loss: 0.555765\n",
      "\nValidation set: Average loss: 0.9716)\n\n",
      "log: Train Epoch: 4 [0/22 samples (0%)]\t Batch Loss: 0.463953\n",
      "\nValidation set: Average loss: 0.9236)\n\n",
      "log: Train Epoch: 5 [0/22 samples (0%)]\t Batch Loss: 0.437029\n",
      "\nValidation set: Average loss: 0.8629)\n\n",
      "log: Train Epoch: 6 [0/22 samples (0%)]\t Batch Loss: 0.392539\n",
      "\nValidation set: Average loss: 0.7863)\n\n",
      "log: Train Epoch: 7 [0/22 samples (0%)]\t Batch Loss: 0.328263\n",
      "\nValidation set: Average loss: 0.7057)\n\n",
      "log: Train Epoch: 8 [0/22 samples (0%)]\t Batch Loss: 0.311477\n",
      "\nValidation set: Average loss: 0.6155)\n\n",
      "log: Train Epoch: 9 [0/22 samples (0%)]\t Batch Loss: 0.271517\n",
      "\nValidation set: Average loss: 0.5407)\n\n",
      "log: Train Epoch: 10 [0/22 samples (0%)]\t Batch Loss: 0.246721\n",
      "\nValidation set: Average loss: 0.4678)\n\n",
      "log: Train Epoch: 11 [0/22 samples (0%)]\t Batch Loss: 0.228121\n",
      "\nValidation set: Average loss: 0.3990)\n\n",
      "log: Train Epoch: 12 [0/22 samples (0%)]\t Batch Loss: 0.216077\n",
      "\nValidation set: Average loss: 0.3413)\n\n",
      "log: Train Epoch: 13 [0/22 samples (0%)]\t Batch Loss: 0.191169\n",
      "\nValidation set: Average loss: 0.2929)\n\n",
      "log: Train Epoch: 14 [0/22 samples (0%)]\t Batch Loss: 0.176861\n",
      "\nValidation set: Average loss: 0.2449)\n\n",
      "log: Train Epoch: 15 [0/22 samples (0%)]\t Batch Loss: 0.164705\n",
      "\nValidation set: Average loss: 0.2058)\n\n",
      "log: Train Epoch: 16 [0/22 samples (0%)]\t Batch Loss: 0.150889\n",
      "\nValidation set: Average loss: 0.1749)\n\n",
      "log: Train Epoch: 17 [0/22 samples (0%)]\t Batch Loss: 0.148772\n",
      "\nValidation set: Average loss: 0.1544)\n\n",
      "log: Train Epoch: 18 [0/22 samples (0%)]\t Batch Loss: 0.142737\n",
      "\nValidation set: Average loss: 0.1508)\n\n",
      "log: Train Epoch: 19 [0/22 samples (0%)]\t Batch Loss: 0.134080\n",
      "\nValidation set: Average loss: 0.1495)\n\n",
      "log: Train Epoch: 20 [0/22 samples (0%)]\t Batch Loss: 0.118421\n",
      "\nValidation set: Average loss: 0.1390)\n\n",
      "log: Train Epoch: 21 [0/22 samples (0%)]\t Batch Loss: 0.127115\n",
      "\nValidation set: Average loss: 0.1222)\n\n",
      "log: Train Epoch: 22 [0/22 samples (0%)]\t Batch Loss: 0.120214\n",
      "\nValidation set: Average loss: 0.1139)\n\n",
      "log: Train Epoch: 23 [0/22 samples (0%)]\t Batch Loss: 0.110053\n",
      "\nValidation set: Average loss: 0.1125)\n\n",
      "log: Train Epoch: 24 [0/22 samples (0%)]\t Batch Loss: 0.115178\n",
      "\nValidation set: Average loss: 0.1123)\n\n",
      "log: Train Epoch: 25 [0/22 samples (0%)]\t Batch Loss: 0.105062\n",
      "\nValidation set: Average loss: 0.1088)\n\n",
      "log: Train Epoch: 26 [0/22 samples (0%)]\t Batch Loss: 0.104249\n",
      "\nValidation set: Average loss: 0.1018)\n\n",
      "log: Train Epoch: 27 [0/22 samples (0%)]\t Batch Loss: 0.092119\n",
      "\nValidation set: Average loss: 0.0950)\n\n",
      "log: Train Epoch: 28 [0/22 samples (0%)]\t Batch Loss: 0.086221\n",
      "\nValidation set: Average loss: 0.0930)\n\n",
      "log: Train Epoch: 29 [0/22 samples (0%)]\t Batch Loss: 0.099194\n",
      "\nValidation set: Average loss: 0.0929)\n\n",
      "log: Train Epoch: 30 [0/22 samples (0%)]\t Batch Loss: 0.096699\n",
      "\nValidation set: Average loss: 0.0908)\n\n",
      "log: Train Epoch: 31 [0/22 samples (0%)]\t Batch Loss: 0.085202\n",
      "\nValidation set: Average loss: 0.0875)\n\n",
      "log: Train Epoch: 32 [0/22 samples (0%)]\t Batch Loss: 0.083699\n",
      "\nValidation set: Average loss: 0.0881)\n\n",
      "log: Train Epoch: 33 [0/22 samples (0%)]\t Batch Loss: 0.076876\n",
      "\nValidation set: Average loss: 0.0886)\n\n",
      "log: Train Epoch: 34 [0/22 samples (0%)]\t Batch Loss: 0.082032\n",
      "\nValidation set: Average loss: 0.0844)\n\n",
      "log: Train Epoch: 35 [0/22 samples (0%)]\t Batch Loss: 0.076272\n",
      "\nValidation set: Average loss: 0.0813)\n\n",
      "log: Train Epoch: 36 [0/22 samples (0%)]\t Batch Loss: 0.076882\n",
      "\nValidation set: Average loss: 0.0798)\n\n",
      "log: Train Epoch: 37 [0/22 samples (0%)]\t Batch Loss: 0.075395\n",
      "\nValidation set: Average loss: 0.0771)\n\n",
      "log: Train Epoch: 38 [0/22 samples (0%)]\t Batch Loss: 0.084430\n",
      "\nValidation set: Average loss: 0.0774)\n\n",
      "log: Train Epoch: 39 [0/22 samples (0%)]\t Batch Loss: 0.072468\n",
      "\nValidation set: Average loss: 0.0770)\n\n",
      "log: Train Epoch: 40 [0/22 samples (0%)]\t Batch Loss: 0.072278\n",
      "\nValidation set: Average loss: 0.0752)\n\n",
      "log: Train Epoch: 41 [0/22 samples (0%)]\t Batch Loss: 0.072361\n",
      "\nValidation set: Average loss: 0.0740)\n\n",
      "log: Train Epoch: 42 [0/22 samples (0%)]\t Batch Loss: 0.070730\n",
      "\nValidation set: Average loss: 0.0745)\n\n",
      "log: Train Epoch: 43 [0/22 samples (0%)]\t Batch Loss: 0.077421\n",
      "\nValidation set: Average loss: 0.0758)\n\n",
      "log: Train Epoch: 44 [0/22 samples (0%)]\t Batch Loss: 0.069582\n",
      "\nValidation set: Average loss: 0.0742)\n\n",
      "log: Train Epoch: 45 [0/22 samples (0%)]\t Batch Loss: 0.070158\n",
      "\nValidation set: Average loss: 0.0741)\n\n",
      "log: Train Epoch: 46 [0/22 samples (0%)]\t Batch Loss: 0.073511\n",
      "\nValidation set: Average loss: 0.0732)\n\n",
      "log: Train Epoch: 47 [0/22 samples (0%)]\t Batch Loss: 0.071070\n",
      "\nValidation set: Average loss: 0.0732)\n\n",
      "log: Train Epoch: 48 [0/22 samples (0%)]\t Batch Loss: 0.069299\n",
      "\nValidation set: Average loss: 0.0731)\n\n",
      "log: Train Epoch: 49 [0/22 samples (0%)]\t Batch Loss: 0.064627\n",
      "\nValidation set: Average loss: 0.0706)\n\n",
      "Training complete.\nExperiment ended. Checkpoints stored =)\n",
      "Experiment ended.\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": [
      "INFO:default-qMU8ExZeJQ:Config:\n",
      "INFO:default-qMU8ExZeJQ:{\n    \"base_dir\": \"./experiment_dir\",\n    \"batch_size\": 10,\n    \"learning_rate\": 0.001,\n    \"log_interval\": 10,\n    \"n_epochs\": 50,\n    \"name\": \"experiment_baseline\",\n    \"rnd_seed\": 2,\n    \"seed\": 2,\n    \"use_cuda\": true\n}\n",
      "INFO:default-qMU8ExZeJQ:Experiment set up.\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 0 [0/22 samples (0%)]\t Batch Loss: 1.110972\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 1.0766)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 1 [0/22 samples (0%)]\t Batch Loss: 0.837393\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 1.0544)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 2 [0/22 samples (0%)]\t Batch Loss: 0.646041\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 1.0195)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 3 [0/22 samples (0%)]\t Batch Loss: 0.555765\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.9716)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 4 [0/22 samples (0%)]\t Batch Loss: 0.463953\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.9236)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 5 [0/22 samples (0%)]\t Batch Loss: 0.437029\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.8629)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 6 [0/22 samples (0%)]\t Batch Loss: 0.392539\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.7863)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 7 [0/22 samples (0%)]\t Batch Loss: 0.328263\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.7057)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 8 [0/22 samples (0%)]\t Batch Loss: 0.311477\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.6155)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 9 [0/22 samples (0%)]\t Batch Loss: 0.271517\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.5407)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 10 [0/22 samples (0%)]\t Batch Loss: 0.246721\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.4678)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 11 [0/22 samples (0%)]\t Batch Loss: 0.228121\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.3990)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 12 [0/22 samples (0%)]\t Batch Loss: 0.216077\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.3413)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 13 [0/22 samples (0%)]\t Batch Loss: 0.191169\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.2929)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 14 [0/22 samples (0%)]\t Batch Loss: 0.176861\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.2449)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 15 [0/22 samples (0%)]\t Batch Loss: 0.164705\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.2058)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 16 [0/22 samples (0%)]\t Batch Loss: 0.150889\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.1749)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 17 [0/22 samples (0%)]\t Batch Loss: 0.148772\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.1544)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 18 [0/22 samples (0%)]\t Batch Loss: 0.142737\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.1508)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 19 [0/22 samples (0%)]\t Batch Loss: 0.134080\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.1495)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 20 [0/22 samples (0%)]\t Batch Loss: 0.118421\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.1390)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 21 [0/22 samples (0%)]\t Batch Loss: 0.127115\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.1222)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 22 [0/22 samples (0%)]\t Batch Loss: 0.120214\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.1139)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 23 [0/22 samples (0%)]\t Batch Loss: 0.110053\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.1125)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 24 [0/22 samples (0%)]\t Batch Loss: 0.115178\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.1123)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 25 [0/22 samples (0%)]\t Batch Loss: 0.105062\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.1088)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 26 [0/22 samples (0%)]\t Batch Loss: 0.104249\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.1018)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 27 [0/22 samples (0%)]\t Batch Loss: 0.092119\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.0950)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 28 [0/22 samples (0%)]\t Batch Loss: 0.086221\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.0930)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 29 [0/22 samples (0%)]\t Batch Loss: 0.099194\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.0929)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 30 [0/22 samples (0%)]\t Batch Loss: 0.096699\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.0908)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 31 [0/22 samples (0%)]\t Batch Loss: 0.085202\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.0875)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 32 [0/22 samples (0%)]\t Batch Loss: 0.083699\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.0881)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 33 [0/22 samples (0%)]\t Batch Loss: 0.076876\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.0886)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 34 [0/22 samples (0%)]\t Batch Loss: 0.082032\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.0844)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 35 [0/22 samples (0%)]\t Batch Loss: 0.076272\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.0813)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 36 [0/22 samples (0%)]\t Batch Loss: 0.076882\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.0798)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 37 [0/22 samples (0%)]\t Batch Loss: 0.075395\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.0771)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 38 [0/22 samples (0%)]\t Batch Loss: 0.084430\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.0774)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 39 [0/22 samples (0%)]\t Batch Loss: 0.072468\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.0770)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 40 [0/22 samples (0%)]\t Batch Loss: 0.072278\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.0752)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 41 [0/22 samples (0%)]\t Batch Loss: 0.072361\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.0740)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 42 [0/22 samples (0%)]\t Batch Loss: 0.070730\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.0745)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 43 [0/22 samples (0%)]\t Batch Loss: 0.077421\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.0758)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 44 [0/22 samples (0%)]\t Batch Loss: 0.069582\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.0742)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 45 [0/22 samples (0%)]\t Batch Loss: 0.070158\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.0741)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 46 [0/22 samples (0%)]\t Batch Loss: 0.073511\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.0732)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 47 [0/22 samples (0%)]\t Batch Loss: 0.071070\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.0732)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 48 [0/22 samples (0%)]\t Batch Loss: 0.069299\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.0731)\n\n",
      "INFO:default-qMU8ExZeJQ:log: Train Epoch: 49 [0/22 samples (0%)]\t Batch Loss: 0.064627\n",
      "INFO:default-qMU8ExZeJQ:\nValidation set: Average loss: 0.0706)\n\n",
      "INFO:default-qMU8ExZeJQ:Experiment ended. Checkpoints stored =)\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "exp.run()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0ce0f76a4b43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mgrid_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfig1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/DUL_ex1/pixelCNN.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, num_samples, img_shape, device)\u001b[0m\n\u001b[1;32m    122\u001b[0m                         \u001b[0mpixelwise_logp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                         \u001b[0mpmf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixelwise_logp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                         \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpmf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/DUL_ex1/pixelCNN.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, num_samples, img_shape, device)\u001b[0m\n\u001b[1;32m    122\u001b[0m                         \u001b[0mpixelwise_logp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                         \u001b[0mpmf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixelwise_logp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m                         \u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpmf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/191.7479.30/helpers/pydev/_pydevd_bundle/pydevd_frame.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mplugin_stop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 723\u001b[0;31m                     \u001b[0mstopped_on_plugin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplugin_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_debugger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_cmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    724\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mis_line\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/191.7479.30/helpers-pro/jupyter_debug/pydev_jupyter_plugin.py\u001b[0m in \u001b[0;36mstop\u001b[0;34m(plugin, pydb, frame, event, args, stop_info, arg, step_cmd)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuspend_jupyter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain_debugger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_cmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mmain_debugger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_wait_suspend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/JetBrains/Toolbox/apps/PyCharm-P/ch-0/191.7479.30/helpers/pydev/pydevd.py\u001b[0m in \u001b[0;36mdo_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, send_suspend_message)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_internal_commands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancel_async_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_thread_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error"
    }
   ],
   "source": [
    "samples = exp.model.sample(9, (10, 10), device=exp.device)\n",
    "\n",
    "grid_len = 3\n",
    "fig1, axes = plt.subplots(grid_len, grid_len, figsize=(20,20))\n",
    "i = 0\n",
    "for ax in axes.flat:\n",
    "    if i >= len(samples):\n",
    "        break\n",
    "    ax.imshow(samples[i].squeeze() / (samples[i].max() + 1e-5), cmap='gray')\n",
    "    ax.axis('off')\n",
    "    i += 1\n",
    "fig1.tight_layout()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}